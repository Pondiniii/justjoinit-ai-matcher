# LLM Configuration
# Base URL for your LLM API (OpenAI-compatible endpoint)
LLM_BASE_URL=https://api.x.ai/v1

# Model name to use
LLM_MODEL=grok-4-fast-non-reasoning

# Request timeout in seconds
LLM_TIMEOUT=180

# API key for LLM service (xAI, OpenAI, etc.)
# Get your key from: https://console.x.ai/
LLM_API_KEY=your-api-key-here

# Database Configuration (used by docker-compose)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=justjoinit

# Optional: Pipeline rate limiting (seconds between requests)
# PIPELINE_RATE_LIMIT=2.0
